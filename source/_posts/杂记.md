```
grpc_cli ls localhost:5010

grpcurl -plaintext -proto grpcService/order/pack.proto localhost:5010 list

```
```
export async function compressImage(buffer, imageType) {
    return await sharp(buffer)
        .jpeg({ quality: 50, mozjpeg: true, progressive: true, force: false })
        .png({ quality: 1, compressionLevel: 2, progressive: true, force: true })
        .toBuffer()
}
```

tinypng.com 密钥： bHhz268xsYYxPmXpBwqJbBjSKdqV8Wrp
curl --user api:bHhz268xsYYxPmXpBwqJbBjSKdqV8Wrp \
--data-binary @101629358726_.pic_hd.jpg -i https://api.tinify.com/shrink

Token防止表单重复提交和CSRF攻击
Token，可以翻译成标记！最大的特点就是随机性，不可预测，一般黑客或软件无法猜测出来。
Token一般用在两个地方:
1: 防止表单重复提交
2: anti csrf攻击（Cross-site request forgery 跨站点请求伪造）
两者在原理上都是通过session token来实现的。当客户端请求页面时，服务器会生成一个随机数Token，并且将Token放置到session当中，然后将Token发给客户端（一般通过构造hidden表单）。
下次客户端提交请求时，Token会随着表单一起提交到服务器端。

1、应用于“anti csrf攻击”：
服务器端会对Token值进行验证，判断是否和session中的Token值相等，若相等，则可以证明请求有效，不是伪造的。

2、应用于“防止表单重复提交”：
服务器端第一次验证相同过后，会将session中的Token值更新下，若用户重复提交，第二次的验证判断将失败，因为用户提交的表单中的Token没变，但服务器端session中Token已经改变了。



```
@Test(expected = ResubmitException.class)
public void errorTest() {
    UserService service = ResubmitProxy.getProxy(new UserService());
    service.queryInfo("1");
    service.queryInfo("1");
}

/**
 * @author binbin.hou
 * @since 0.0.1
 */
@Retention(RetentionPolicy.RUNTIME)
@Target(ElementType.METHOD)
@Documented
public @interface Resubmit {

    /**
     * 缓存实现策略
     * @return 实现
     * @since 0.0.1
     */
    Class cache() default ICache.class;

    /**
     * key 生成策略
     * @return 生成策略
     * @since 0.0.1
     */
    Class keyGenerator() default IKeyGenerator.class;

    /**
     * 密匙生成策略
     * @return 生成策略
     * @since 0.0.1
     */
    Class tokenGenerator() default ITokenGenerator.class;

    /**
     * 存活时间
     *
     * 单位：秒
     * @return 时间
     * @since 0.0.1
     */
    int ttl() default 60;

}


public class ConcurrentHashMapCache implements ICache {

    /**
     * 日志信息
     * @since 0.0.1
     */
    private static final Log LOG = LogFactory.getLog(ConcurrentHashMapCache.class);

    /**
     * 存储信息
     * @since 0.0.1
     */
    private static final ConcurrentHashMap MAP = new ConcurrentHashMap<>();

    static {
        Executors.newScheduledThreadPool(1)
            .scheduleAtFixedRate(new CleanTask(), 1, 1,
                    TimeUnit.SECONDS);
    }

    /**
     * 清理任务执行
     * @since 0.0.1
     */
    private static class CleanTask implements Runnable {
        @Override
        public void run() {
            LOG.info("[Cache] 开始清理过期数据");

            // 当前时间固定，不需要考虑删除的耗时
            // 毕竟最多相差 1s，但是和系统的时钟交互是比删除耗时多的。
            long currentMills = System.currentTimeMillis();

            for(Map.Entry entry : MAP.entrySet()) {
                long live = entry.getValue();
                if(currentMills >= live) {
                    final String key = entry.getKey();
                    MAP.remove(key);
                    LOG.info("[Cache] 移除 key: {}", key);
                }
            }

            LOG.info("[Cache] 结束清理过期数据");
        }
    }

    @Override
    public void put(String key, int ttlSeconds) {
        if(ttlSeconds <= 0) {
            LOG.info("[Cache] ttl is less than 1, just ignore.");
            return;
        }
        long time = System.currentTimeMillis();
        long liveTo = time + ttlSeconds * 1000;

        LOG.info("[Cache] put into cache, key: {}, live to: {}", key, liveTo);
        MAP.putIfAbsent(key, liveTo);
    }

    @Override
    public boolean contains(String key) {
        boolean result =  MAP.containsKey(key);

        LOG.info("[Cache] contains key: {} result: {}", key, result);
        return result;
    }

}

/**
 * CGLIB 代理类
 * @author binbin.hou
 * date 2019/3/7
 * @since 0.0.2
 */
public class CglibProxy implements MethodInterceptor, IResubmitProxy {

    /**
     * 被代理的对象
     */
    private final Object target;

    public CglibProxy(Object target) {
        this.target = target;
    }

    @Override
    public Object intercept(Object o, Method method, Object[] objects, MethodProxy methodProxy) throws Throwable {
        //1. 添加判断
        ResubmitProxy.resubmit(method, objects);

        //2. 返回结果
        return method.invoke(target, objects);
    }

    @Override
    public Object proxy() {
        Enhancer enhancer = new Enhancer();
        //目标对象类
        enhancer.setSuperclass(target.getClass());
        enhancer.setCallback(this);
        //通过字节码技术创建目标对象类的子类实例作为代理
        return enhancer.create();
    }

}


/**
 * 重复提交验证
 * @param method 方法
 * @param args 入参
 * @since 0.0.1
 */
public static void resubmit(final Method method,
                            final Object[] args) {
    if(method.isAnnotationPresent(Resubmit.class)) {
        Resubmit resubmit = method.getAnnotation(Resubmit.class);
        // 构建入参
        ResubmitBs.newInstance()
                .cache(resubmit.cache())
                .ttl(resubmit.ttl())
                .keyGenerator(resubmit.keyGenerator())
                .tokenGenerator(resubmit.tokenGenerator())
                .method(method)
                .params(args)
                .resubmit();
    }
}


```

https://github.com/saadeghi/daisyui

按业务负载特征，关系型数据库可分为 OLTP 数据库（交易型）和 OLAP 数据库（分析型) ：

OLTP，Online Transaction Processing。OLTP 数据库的特点是支持事务，增删查改等功能强大，适合需要被频繁修改的"热数据"。我们耳熟能详的 Mysql、Pg 等都属于这一类。缺点就是由于支持事务，插入时比较慢。拿来实现我们的需求显然是不合适的。

OLAP，Online Analytical Processing，数据分析为主。不支持事务，或者说是对事务的支持有限。OLAP 的场景是：大多数是读请求，数据总是以相当大的批(> 1000 rows)进行写入，不修改已添加的数据。


```
function x-git-update-all-project() {
    for project in $(ls $1)
    do
        if [[ $project == $2 ]]; then
            ## 要例外的不更新的项目名
        else
            cd $1/$project
            now=$(date +"%y%m%d%H%M")
            git stash
            git checkout -b tmp-update-$now origin/master
            git branch -D master
            git fetch --all
            git checkout -t origin/master
            git branch -D tmp-update-$now
            git fetch -p
        fi
    done
}


function get-pull-all-project() {
    ls | xargs -I{} git -C {} pull
    # 并行执行
    # ls | xargs -P10 -I{} git -C {} pull
}
```

{
  "name": "a605876186fc8566",
  "password": "cf4712b4f535d09d"
}





服务端渲染SSR(Nuxt.js/Next.js),前端是vue向,后者是react向
原生应用(Weex/React Native)
小程序(mpvue/uni-app)等



https://github.com/puikinsh/CoolAdmin



docker run --name repo alpine/git clone \ https://github.com/docker/getting-started.git

docker cp repo:/git/getting-started/ .

cd getting-started

docker build -t docker101tutorial .

docker run -d -p 80:80 \ 
--name docker-tutorial docker101tutorial

docker tag docker101tutorial {userName}/docker101tutorial
docker push {userName}/docker101tutorial

brew install act  tree

```js
module.exports =(env) => { return require(`./webpack.${env}.js`) }

webpack.dev.js or webpack.prod.js

{
  "scripts": { 
    "dev": "webpack-dev-server --inline --hot --no-info --env dev",
    "build:dev": "webpack --env dev --progress --profile --colors", 
    "build:prod": "webpack --env prod --progress --profile --colors", },
}

  
webpack --env dev --progress --profile --colors


Object.values() and Object.keys()
const ingredients = {
    eggs: 4,
    lemons: 2,
    sugar: '225g',
    flour: '225g',
    butter: '180g'
}

const ul = document.querySelector('.ingredients');
Object.keys(ingredients).forEach(ingredient =>
  ul.innerHTML += `<li>${ingredients[ingredient]} ${ingredient}</li>`
)

const ul = document.querySelector('.ingredients');
Object.entries(ingredients).forEach(([key, val]) => {
    ul.innerHTML += `<li>${val} ${key}</li>`;
});



[
    {
        "name": "jim",
        "color": "blue",
        "age": "22"
    },
    {
        "name": "Sam",
        "color": "blue",
        "age": "33"
    },
    {
        "name": "eddie",
        "color": "green",
        "age": "77"
    }
]


[
    {
        color: "blue",
        users: [
            {
                "name": "jim",
                "color": "blue",
                "age": "22"
            },
            {
                "name": "Sam",
                "color": "blue",
                "age": "33"
            }
        ]
    },
    {
        color: "green",
        users: [
            {
                "name": "eddie",
                "color": "green",
                "age": "77"
            }
        ]
    }
]


console.log(
  _.chain(data)
    // Group the elements of Array based on `color` property
    .groupBy("color")
    // `key` is group's name (color), `value` is the array of objects
    .map((value, key) => ({ color: key, users: value }))
    .value()
);


var result = _.chain(data)
    .groupBy("color")
    .pairs()
    .map(function(currentItem) {
        return _.object(_.zip(["color", "users"], currentItem));
    })
    .value();
console.log(result);
注: Lodash 4.0 以后，.pairs函数已重命名为 _.toPairs()

_.chain(data)
  .groupBy("color")
  .toPairs()
  .map(item => _.zipObject(["color", "users"], item))
  .value();

var result = _(data)
            .groupBy(x => x.color)
            .map((value, key) => ({color: key, users: value}))
            .value();

_.chain(data)
    .groupBy('color')
    .map((users, color) => ({ users, color }))
    .value();

var result = groupBy(data, 'color', 'colorId', 'users');

const data = [{ "name": "jim","color": "blue","age": "22"},{"name": "Sam", "color": "blue", "age": "33" }, { "name":"eddie","color": "green","age": "77"}];

const result = data.reduce((acc, {name, color, age}) => {
 acc[color] ??= {color: color, users: []};
 acc[color].users.push({name, color, age});

 return acc;
}, {});
console.log(Object.values(result));

const cars = [{
make: "audi",
model: "r8",
year: "2012"
},
{
make: "audi",
model: "rs5",
year: "2013"
},
{
make: "ford",
model: "mustang",
year: "2012"
},
{
make: "ford",
model: "fusion",
year: "2015"
},
{
make: "kia",
model: "optima",
year: "2012"
}];

let group = cars.reduce((r, a) => {
 console.log("a", a);
 console.log('r', r);
 r[a.make] = [...r[a.make] || [], a];
 return r;
}, {});
console.log("group", group);


for (const [i, v] of arr.entries()) {
  console.log(i, v); // Prints "0 a", "1 b", "2 c"
}

const groupedMap = initialArray.reduce(
    (entryMap, e) => entryMap.set(e.id, [...entryMap.get(e.id)||[], e]),
    new Map()
);

Map是一个数组数组更有用的结果。但是如果你想要一个数组数组，那么你可以调用Array.from(groupedMap.entries())(对于一个[key, group array]对的数组)或Array.from(groupedMap.values())(对于一个简单的数组数组)。

const mergedArray = Array.from(
    objsToMerge.reduce(
        (entryMap, e) => entryMap.set(e.id, {...entryMap.get(e.id)||{}, ...e}),
        new Map()
    ).values()
);

Array.prototype.groupBy = function(keyFunction) {
    var groups = {};
    this.forEach(function(el) {
        var key = keyFunction(el);
        if (key in groups == false) {
            groups[key] = [];
        }
        groups[key].push(el);
    });
    return Object.keys(groups).map(function(key) {
        return {
            key: key,
            values: groups[key]
        };
    });
};


moment.utc('2021-07-01').add(1, 'days').format('YYYY-MM-DD HH:mm:ss')
moment.utcOffset(-7).format('YYYY-MM-DD HH:mm:ss')

{
  rooms: Array<{
    capacity: number,
    name: string,
    assets: Array<string>,
  }>,
  bookings: Array<{
    room: string, // room ID
    bookingStart: string, // ISO String
    bookingEnd: string, //ISO string
    status: 'RESERVED' | 'CANCELLED',
    cancelReason?: string, // exists when status === 'canceled
    purpose: string,
    user: {
      name: string,
      email: string,
      phone: string
    }
  }>,
}

docker container prune
docker image prune
```



```yaml
name: 'Train-in-the-cloud'
on: [push]

jobs:
  deploy-runner:
    runs-on: [ubuntu-latest]
    steps:
      - uses: iterative/setup-cml@v1
      - uses: actions/checkout@v2
      - name: 'Deploy runner on EC2'
        shell: bash
        env:
          REPO_TOKEN: ${{ secrets.PERSONAL_ACCESS_TOKEN }}
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        run: |
          cml-runner \
          --cloud aws \
          --cloud-region us-west \
          --cloud-type=t2.micro \
          --labels=cml-runner
  model-training:
    needs: deploy-runner
    runs-on: [self-hosted, cml-runner]
    container: docker://iterativeai/cml:0-dvc2-base1
    steps:
      - uses: actions/checkout@v2
      - name: 'Train my model'
        env:
          repo_token: ${{ secrets.PERSONAL_ACCESS_TOKEN }}
        run: |
          pip install -r requirements.txt
          python train.py

          # Publish report with CML
          cat metrics.txt > report.md
          cml-send-comment report.md
```

<details>
  <summary>nodejs install</summary>
  - Mac 
  brew install
  - Linux
  apt install nodejs
</details>





```js
type FieldNames = "Field One" | "Field Two";

let x: FieldNames;

x = "Field One";
x = "Field Two";

// Error - not allowed
x = "Field Three";
```

Error: 14 UNAVAILABLE: TCP Read failed

I found solution at http://www.longfan.me/post/devops/2020-07-09
I add at nestjs ClientOptions

```
  options: {
   keepalive: {
      keepaliveTimeMs: 900,
      keepalivePermitWithoutCalls: 1,
      http2MaxPingsWithoutData: 0,
    },
   ...
```

and it solve this issue



```js
// server.js
const { createServer } = require('http')
const io = require('socket.io')
const addSocketEvents = require('./socket-api')

const port = process.env.PORT || 8080
const proxyPort = 8081

module.exports = {
    port,
    proxyPort,
    configure(app) {
        // `app` is an instance of express

        // add the websockets
        let httpServer, socketPort
        if (process.env.NODE_ENV === 'development') {
            httpServer = createServer()
            socketPort = proxyPort
        } else {
            httpServer = createServer(app)
            socketPort = port
        }

        // adds the socket-api to be used via websockets
        socket = io(httpServer, {
            path: '/socket-api'
        })
        addSocketEvents(socket)
        
        // starts the server
        httpServer.listen(socketPort)
    }
}
// vue.config.js
const { port, configure } = require('./server')

module.exports = {
    devServer: {
        before: configure,
        public: `localhost:${port}`,
        proxy: {
            '/socket-api': {
                target: `http://localhost:${proxyPort}`,
                ws: true
            }
        }
    },
}
```



### gRPC 'Error: 14 UNAVAILABLE: TCP Write failed' issue

http://www.longfan.me/post/devops/2020-07-09

https://github.com/grpc/grpc-node/issues/1432







```bash
docker run -it \
  -e DATABASE_CLIENT=postgres \
  -e DATABASE_NAME=strapi \
  -e DATABASE_HOST=0.0.0.0 \
  -e DATABASE_PORT=5432 \
  -e DATABASE_USERNAME=strapi \
  -e DATABASE_PASSWORD=strapi \
  -p 1337:1337 \
  -v `pwd`/project-name:/srv/app \
  strapi/strapi
```



``docker run --publish 3085:3085 -it codaprotocol/coda-demo``

webcal://p72-caldav.icloud.com.cn/published/2/MTAxNjAxOTU0MTAxMDE2MFbNg2j0mKCh6iJBzLBJyzu_P7TbJro-mN003BdLtL7z9tgJciQfNA1rAigHWAL2VnADhH2HHYRh8IrAGrF4F_A

webcal://p72-caldav.icloud.com.cn/published/2/MTAxNjAxOTU0MTAxMDE2MFbNg2j0mKCh6iJBzLBJyzt5MrjLsMgTdDskarsyJQLWS9YjxnWVnwgWxxi1oys-uvLdJWDt8ss8D-EyYiL1-mc

```sh
aws s3 cp ./dist s3://better-cover-letter  --recursive --exclude "*.DS_Store" --acl public-read --cache-control public,max-age=604800 --dryrun --profile iam_user

aws s3 rm s3://better-cover-letter/assets --recursive --profile iam_user --dryrun

aws s3 sync dist s3://[YOUR_BUCKET_NAME] --delete

aws cloudfront create-invalidation --distribution-id [distribution_ID] --paths "/*"
```



```json
# package.json
...
"scripts": {
  ...
  "deploy": "webpack -p --config webpack.config.prod.js && aws s3 cp ./dist s3://better-cover-letter  --recursive --exclude "*.DS_Store" --cache-control public,max-age=604800 --dryrun --profile iam_user"
  ...
}
```



https://codepen.io/

https://www.gitpod.io/

光伏

先进制造

https://gitpod.io/workspaces

Jira UI  https://jira.hyperledger.org/browse/FAB-17131



esbuild

https://github.com/harsima/vue-backend

https://github.com/sisterAn/JavaScript-Algorithms

https://github.com/verekia/js-stack-boilerplate

https://github.com/verekia/js-stack-from-scratch

extract title and tag from url  http://tools.buzzstream.com/meta-tag-extractor

https://github.com/CyC2018/CS-Notes

https://github.com/CyC2018/Job-Recommend

[General purpose task for publishing files to a gh-pages branch on GitHub](https://github.com/tschaub/gh-pages)

https://github.com/suweiteng/vue2-management-platform

https://github.com/microsoft/Web-Dev-For-Beginners

https://github.com/element-plus/element-plus

https://github.com/typicode/lowdb

https://github.com/effector/effector

https://github.com/effector/logger

https://github.com/MinaProtocol/mina

https://github.com/chatwoot/chatwoot

[![img](https://s4.51cto.com/oss/202012/10/2c97d3cc72c50c20ea875d9e395c6baf.jpg)](https://s4.51cto.com/oss/202012/10/2c97d3cc72c50c20ea875d9e395c6baf.jpg)

这是一个即时聊天软件ChatWoot，通过ChatWoot可以实现网站访客与网站主人的即时通讯。

https://github.com/Marak/faker.js

[![img](https://s2.51cto.com/oss/202012/10/d0e03b355c16082cb280f8268498c0d8.jpg)](https://s2.51cto.com/oss/202012/10/d0e03b355c16082cb280f8268498c0d8.jpg)

Faker.js是一个JavaScript库，每周下载量超过140万。大概八年前就已经被创建了，如今有200多个贡献者。它公开了生成随机数据的函数，通常调用这些函数就可以生成JSON或CSV文件的值，而且数据生成的速度很快，并且易于使用。

https://github.com/hug-sun/element3

这是适用于Web的Vue.js 3.0 UI工具包。旨在帮助开发者学习vue3的社区版，会有配套的实战教程和B站教学视频

https://github.com/prysmaticlabs/prysm

Prysm是以太坊2.0客户端，这个项目是Prysm的核心存储库，由Prysmatic Labs开发。

https://github.com/hyj1991/easy-monitor

https://github.com/juliangarnier/anime JavaScript animation engine

https://github.com/jef/streetmerchant

StreetMerchant是强大的电商库存检测器，可以在你开展日常业务时，让刷新和检查网站库存变得更加轻松。具有以下功能特性：

- 通过API和Chromium模拟访问多个网站以查找库存
- 有库存时打开浏览器
- 可用时发送通知的能力
- 机器人不会自动为您购买

https://github.com/AMAI-GmbH/AI-Expert-Roadmap

https://github.com/mirari/v-viewer

https://github.com/mrmlnc/fast-glob

https://github.com/sdras/awesome-actions

https://github.com/KeKe-Li/tutorial

https://github.com/KeKe-Li/books

https://github.com/KeKe-Li/data-structures-questions

https://github.com/verekia/js-stack-from-scratch

https://github.com/lowerfish/js-stack-from-scratch

https://github.com/vuejs/vitepress

https://github.com/Tencent/secguide

https://github.com/Tencent/secguide/blob/main/JavaScript安全指南.md

https://github.com/swc-project/swc 快速编译ts



https://github.com/GoogleChromeLabs/squoosh 图片压缩

https://github.com/GoogleChromeLabs/ndb  ndb is an improved debugging experience for Node.js, enabled by Chrome DevTools

https://github.com/dotansimha/graphql-code-generator

https://github.com/kamilkisiela/graphql-inspector

https://github.com/Urigo/graphql-mesh

https://github.com/ardatan/graphql-tools

https://the-guild.dev/services

https://github.com/yarax/swagger-to-graphql

https://github.com/IBM/openapi-to-graphql

https://github.com/MichalLytek/type-graphql

https://github.com/kamilkisiela/graphql-config

https://github.com/nocodb/nocodb

https://github.com/fivethree-team/nestjs-prisma-starter

https://github.com/ERS-HCL/nxplorerjs-microservice-starter#graphql

https://graphcms.com/

https://github.com/kriasoft/graphql-starter

https://github.com/kriasoft/react-starter-kit

https://github.com/APIs-guru/graphql-voyager

https://github.com/dotansimha/graphql-yoga

https://github.com/howtographql/howtographql

https://github.com/graphql/graphql-spec

https://github.com/chentsulin/awesome-graphql

https://github.com/graphql/graphql.github.io

https://github.com/MichalLytek/type-graphql

https://github.com/graphql/express-graphql

https://github.com/graphql/graphiql

https://github.com/parse-community/parse-server

https://github.com/marmelab/react-admin

https://github.com/marmelab/json-graphql-server

https://github.com/marmelab/awesome-rest

https://github.com/swinton/github-rest-apis-for-insomnia

https://github.com/jozsefsallai/insomnia-documenter

https://github.com/mlabouardy/swaggymnia

https://github.com/mirumee/saleor

https://github.com/spree/spree

https://github.com/reactioncommerce/reaction

https://github.com/directus/directus

https://github.com/prisma/prisma

https://github.com/graphql/dataloader

https://github.com/graphile/postgraphile

https://github.com/relax/relax

https://github.com/mswjs/msw

https://github.com/vitejs/vite

https://github.com/anncwb/vue-vben-admin

https://github.com/service-mocker/service-mocker

https://github.com/artf/grapesjs

https://github.com/nextapps-de/flexsearch

https://github.com/airbnb/knowledge-repo

https://github.com/go-gitea/gitea

https://github.com/froala/wysiwyg-editor⭐️

https://github.com/froala/design-blocks ⭐️

https://github.com/n457/Uncolored (Un)colored — Next generation desktop rich content editor that saves documents with themes. HTML & Markdown compatible. For Windows, OS X & Linux. —

https://github.com/smapiot/piral

https://github.com/vercel/commerce ⭐️

https://github.com/reactioncommerce/reaction

https://github.com/mirumee/saleor⭐️

https://github.com/mirumee/saleor-dashboard

https://github.com/spree/spree

https://github.com/PrestaShop/PrestaShop

https://github.com/pimcore/pimcore ⭐️

保存两次，只编译最后一次可以吗

重新编译时，axios频繁请求

 编辑页面添加规格时，只能添加一个值

**逻辑或（||）总结：**

1. 只要第一个值的布尔值为false，那么永远返回第二个值。
2. 逻辑或属于短路操作，第一个值为true时，不再操作第二个值，且返回第一个值。

**逻辑与（&&）总结：**

1. 只要第一个值的布尔值为true，那么永远返回第二个值。
2. 逻辑与属于短路操作，第一个值为false时，不再操作第二个值，且返回第一个值。



作者：Rin阳
链接：https://www.jianshu.com/p/07a1cabe6484
来源：简书
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。



```json
// vscode setting

{
    "workbench.colorTheme": "Quiet Light",
    "window.zoomLevel": 2,
    "files.exclude": {
        "**/node_modules": true
    },
    "workbench.iconTheme": "vscode-icons",
    "editor.fontLigatures": true,
    "editor.fontFamily": "'Fira Code', Menlo, Monaco, 'Courier New', monospace",
    "editor.minimap.enabled": false,
    "editor.renderWhitespace": "none",
    "editor.renderControlCharacters": true,
    "js/ts.implicitProjectConfig.experimentalDecorators": true,
    "cSpell.userWords": [
        "ciphertext",
        "graphicsmagick",
        "grpc",
        "Imgs",
        "slashare"
    ],
    "terminal.integrated.sendKeybindingsToShell": true
}
```



##  Good node modules

https://github.com/philipstanislaus/performant-array-to-tree







## 解决build后的网站访问404，但页面正常

```nginx
# Ngnix解决方案
location / {
    root ...
    index ...
    try_files $uri $uri/ /index.html; ---解决页面刷新404问题
}
```

http-server --gzip --proxy http://localhost:8080?

AWS CloudFront解决方案

使用**AWS CloudFront** > **CloudFront分配**>存储桶的分配>**错误页面，**然后添加所需的错误代码。如果您不使用`react-router`（例如下面的示例），则存储桶将响应**Error 403，**因此您可以使用*error.html*进行响应。

在CloudFront分发中创建一个[自定义错误页面（AWS文档）](https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/custom-error-pages.html)，该[页面](https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/custom-error-pages.html)将404错误路由到index.html并返回200响应代码。这样，您的应用程序将处理路由。

将4xx重定向到index.html可以，但是该解决方案会使您陷入许多其他问题，例如google无法抓取这些页面。请检查[此链接](https://viastudio.com/hosting-a-reactjs-app-with-routing-on-aws-s3/)，它通过使用S3存储桶的重定向规则解决了此问题。



```sh
$ sudo npx http-server ./build -p 433 -a whatever.com -S -C ./ssl/_wildcard.pem -K ./ssl/_wildcard.key.pem -c-1

$ rm -f ./node_modules/webpack-dev-server/ssl/server.pem && cat ./ssl/server.pem > ./node_modules/webpack-dev-server/ssl/server.pem

```



```yaml
name: Gatsby Publish

on:
  push:
    branches:
      - dev

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v1
      - uses: enriikke/gatsby-gh-pages-action@v2
        with:
          access-token: ${{ secrets.ACCESS_TOKEN }}


name: 'Run tests'
on:
  - push
  - pull_request
jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - uses: actions/setup-node@v2
        with:
          node-version: 16
      - run: npm ci
      - run: npm run lint
      - run: npm test
      - run: npm run test:coverage
      - run: npm run test:coverage:report
      - name: Coveralls
        uses: coverallsapp/github-action@1.1.3
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}


### Check your applications for vulnerabilties using Sny
- name: Snyk 
  uses: snyk/actions@0.3.0
  
  
  
name: Example workflow using Snyk
on: push
jobs:
  security:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@master
      - name: Run Snyk to check for vulnerabilities
        uses: snyk/actions/node@master
        env:
          SNYK_TOKEN: ${{ secrets.SNYK_TOKEN }}
        with:
          command: monitor
          
          
name: Snyk example
on: push
jobs:
  security:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@master
      - uses: snyk/actions/setup@master
      - uses: actions/setup-go@v1
        with:
          go-version: '1.13'
      - name: Snyk monitor
        run: snyk test
        env:
          SNYK_TOKEN: ${{ secrets.SNYK_TOKEN }}
          
          
name: Example workflow using Snyk with continue on error
on: push
jobs:
  security:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@master
      - name: Run Snyk to check for vulnerabilities
        uses: snyk/actions/node@master
        continue-on-error: true
        env:
          SNYK_TOKEN: ${{ secrets.SNYK_TOKEN }}
     
steps:
- uses: actions/checkout@v2
- uses: actions/setup-node@v2
  with:
    node-version: '14'
    cache: 'npm'
    cache-dependency-path: subdir/package-lock.json
- run: npm install
- run: npm test


jobs:
  build:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        node: [ '12', '14', '16' ]
    name: Node ${{ matrix.node }} sample
    steps:
      - uses: actions/checkout@v2
      - name: Setup node
        uses: actions/setup-node@v2
        with:
          node-version: ${{ matrix.node }}
      - run: npm install
      - run: npm test

steps:
- uses: actions/checkout@v2
- uses: actions/setup-node@v2
  with:
    node-version: '14'
    cache: 'yarn'
- run: yarn install
- run: yarn test

# Multiple Operating Systems and Architectures
jobs:
  build:
    runs-on: ${{ matrix.os }}
    strategy:
      matrix:
        os:
          - ubuntu-latest
          - macos-latest
          - windows-latest
        node_version:
          - 12
          - 14
          - 16
        architecture:
          - x64
        # an extra windows-x86 run:
        include:
          - os: windows-2016
            node_version: 12
            architecture: x86
    name: Node ${{ matrix.node_version }} - ${{ matrix.architecture }} on ${{ matrix.os }}
    steps:
      - uses: actions/checkout@v2
      - name: Setup node
        uses: actions/setup-node@v2
        with:
          node-version: ${{ matrix.node_version }}
          architecture: ${{ matrix.architecture }}
      - run: npm install
      - run: npm test

#Publish to npmjs and GPR with npm
steps:
- uses: actions/checkout@v2
- uses: actions/setup-node@v2
  with:
    node-version: '14.x'
    registry-url: 'https://registry.npmjs.org'
- run: npm install
- run: npm publish
  env:
    NODE_AUTH_TOKEN: ${{ secrets.NPM_TOKEN }}
- uses: actions/setup-node@v2
  with:
    registry-url: 'https://npm.pkg.github.com'
- run: npm publish
  env:
    NODE_AUTH_TOKEN: ${{ secrets.GITHUB_TOKEN }}

# Publish to npmjs and GPR with yarn
steps:
- uses: actions/checkout@v2
- uses: actions/setup-node@v2
  with:
    node-version: '14.x'
    registry-url: <registry url>
- run: yarn install
- run: yarn publish
  env:
    NODE_AUTH_TOKEN: ${{ secrets.YARN_TOKEN }}
- uses: actions/setup-node@v2
  with:
    registry-url: 'https://npm.pkg.github.com'
- run: yarn publish
  env:
    NODE_AUTH_TOKEN: ${{ secrets.GITHUB_TOKEN }}


# Use private packages
steps:
- uses: actions/checkout@v2
- uses: actions/setup-node@v2
  with:
    node-version: '14.x'
    registry-url: 'https://registry.npmjs.org'
# Skip post-install scripts here, as a malicious
# script could steal NODE_AUTH_TOKEN.
- run: npm install --ignore-scripts
  env:
    NODE_AUTH_TOKEN: ${{ secrets.NPM_TOKEN }}
# `npm rebuild` will run all those post-install scripts for us.
- run: npm rebuild && npm run prepare --if-present


name: Compress Images
on:
  pull_request:
    # Run Image Actions when JPG, JPEG, PNG or WebP files are added or changed.
    # See https://help.github.com/en/actions/automating-your-workflow-with-github-actions/workflow-syntax-for-github-actions#onpushpull_requestpaths for reference.
    paths:
      - '**.jpg'
      - '**.jpeg'
      - '**.png'
      - '**.webp'
jobs:
  build:
    # Only run on Pull Requests within the same repository, and not from forks.
    if: github.event.pull_request.head.repo.full_name == github.repository
    name: calibreapp/image-actions
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Repo
        uses: actions/checkout@v2

      - name: Compress Images
        uses: calibreapp/image-actions@main
        with:
          # The `GITHUB_TOKEN` is automatically generated by GitHub and scoped only to the repository that is currently running the action. By default, the action can’t update Pull Requests initiated from forked repositories.
          # See https://docs.github.com/en/actions/reference/authentication-in-a-workflow and https://help.github.com/en/articles/virtual-environments-for-github-actions#token-permissions
          githubToken: ${{ secrets.GITHUB_TOKEN }}



# Compress images on demand (workflow_dispatch), and at 11pm every Sunday (schedule).
# Open a Pull Request if any images can be compressed.
name: Compress Images
on:
  workflow_dispatch:
  schedule:
    - cron: '00 23 * * 0'
jobs:
  build:
    name: calibreapp/image-actions
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Repo
        uses: actions/checkout@v2
      - name: Compress Images
        id: calibre
        uses: calibreapp/image-actions@main
        with:
          githubToken: ${{ secrets.GITHUB_TOKEN }}
          compressOnly: true
      - name: Create New Pull Request If Needed
        if: steps.calibre.outputs.markdown != ''
        uses: peter-evans/create-pull-request@v3
        with:
          title: Compressed Images Nightly
          branch-suffix: timestamp
          commit-message: Compressed Images
          body: ${{ steps.calibre.outputs.markdown }}


name: Compress Images on Push to main branch
on:
  push:
    branches:
      - main
    paths:
      - '**.jpg'
      - '**.jpeg'
      - '**.png'
      - '**.webp'
jobs:
  build:
    name: calibreapp/image-actions
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Repo
        uses: actions/checkout@v2
      - name: Compress Images
        id: calibre
        uses: calibreapp/image-actions@main
        with:
          githubToken: ${{ secrets.GITHUB_TOKEN }}
          compressOnly: true
      - name: Create New Pull Request If Needed
        if: steps.calibre.outputs.markdown != ''
        uses: peter-evans/create-pull-request@v3
        with:
          title: Compressed Images
          branch-suffix: timestamp
          commit-message: Compressed Images
          body: ${{ steps.calibre.outputs.markdown }}


# Image Actions will run in the following scenarios:
# - on Pull Requests containing images (not including forks)
# - on pushing of images to `main` (for forks)
# - on demand (https://github.blog/changelog/2020-07-06-github-actions-manual-triggers-with-workflow_dispatch/)
# - at 11 PM every Sunday in anything gets missed with any of the above scenarios
# For Pull Requests, the images are added to the PR.
# For other scenarios, a new PR will be opened if any images are compressed.
name: Compress images
on:
  pull_request:
    paths:
      - '**.jpg'
      - '**.jpeg'
      - '**.png'
      - '**.webp'
  push:
    branches:
      - main
    paths:
      - '**.jpg'
      - '**.jpeg'
      - '**.png'
      - '**.webp'
  workflow_dispatch:
  schedule:
    - cron: '00 23 * * 0'
jobs:
  build:
    name: calibreapp/image-actions
    runs-on: ubuntu-latest
    # Only run on main repo on and PRs that match the main repo.
    if: |
      github.repository == 'example/example_repo' &&
      (github.event_name != 'pull_request' ||
       github.event.pull_request.head.repo.full_name == github.repository)
    steps:
      - name: Checkout Branch
        uses: actions/checkout@v2
      - name: Compress Images
        id: calibre
        uses: calibreapp/image-actions@main
        with:
          githubToken: ${{ secrets.GITHUB_TOKEN }}
          # For non-Pull Requests, run in compressOnly mode and we'll PR after.
          compressOnly: ${{ github.event_name != 'pull_request' }}
      - name: Create Pull Request
        # If it's not a Pull Request then commit any changes as a new PR.
        if: |
          github.event_name != 'pull_request' &&
          steps.calibre.outputs.markdown != ''
        uses: peter-evans/create-pull-request@v3
        with:
          title: Auto Compress Images
          branch-suffix: timestamp
          commit-message: Compress Images
          body: ${{ steps.calibre.outputs.markdown }}



# This workflow will do a clean install of node dependencies, cache/restore them, build the source code and run tests across different versions of node
# For more information see: https://help.github.com/actions/language-and-framework-guides/using-nodejs-with-github-actions

name: Node.js CI

on:
  push:
    branches: [ master ]
  pull_request:
    branches: [ master ]

jobs:
  build:

    runs-on: ubuntu-latest

    strategy:
      matrix:
        node-version: [12.x, 14.x, 16.x]
        # See supported Node.js release schedule at https://nodejs.org/en/about/releases/

    steps:
    - uses: actions/checkout@v2
    - name: Use Node.js ${{ matrix.node-version }}
      uses: actions/setup-node@v2
      with:
        node-version: ${{ matrix.node-version }}
        cache: 'npm'
    - run: npm ci
    - run: npm run build --if-present
    - run: npm test

on:
  schedule:
    - cron:  '0 7 * * 1,4'
    # scheduled at 07:00 every Monday and Thursday

  workflow_dispatch:  # click the button on Github repo!

jobs:
  sync_latest_from_upstream:
    runs-on: ubuntu-latest
    name: Sync latest commits from upstream repo

    steps:
    # REQUIRED step
    # Step 1: run a standard checkout action, provided by github
    - name: Checkout target repo
      uses: actions/checkout@v2
      with:
        # optional: set the branch to checkout,
        # sync action checks out your 'target_sync_branch' anyway
        ref:  my-branch
        # REQUIRED if your upstream repo is private (see wiki)
        persist-credentials: false

    # REQUIRED step
    # Step 2: run the sync action
    - name: Sync upstream changes
      id: sync
      uses: aormsby/Fork-Sync-With-Upstream-action@v3.0
      with:
        target_sync_branch: my-branch
        # REQUIRED 'target_repo_token' exactly like this!
        target_repo_token: ${{ secrets.GITHUB_TOKEN }}
        upstream_sync_branch: main
        upstream_sync_repo: aormsby/Fork-Sync-With-Upstream-action
        upstream_repo_access_token: ${{ secrets.UPSTREAM_REPO_SECRET }}

        # Set test_mode true to run tests instead of the true action!!
        test_mode: true
      
    # Step 3: Display a sample message based on the sync output var 'has_new_commits'
    - name: New commits found
      if: steps.sync.outputs.has_new_commits == 'true'
      run: echo "New commits were found to sync."
    
    - name: No new commits
      if: steps.sync.outputs.has_new_commits == 'false'
      run: echo "There were no new commits."
      
    - name: Show value of 'has_new_commits'
      run: echo ${{ steps.sync.outputs.has_new_commits }}










```



1、前端-自行注册
2、商户后台：账号：18645261040    密码：slashare123456,.
3、系统后台：账号：tgq    密码：tgq

shopify网址：https://shopify.com/
登录账号：slashare21@gmail.com
登录密码：Xipin2021*

Q

ssh -i ~/pem/slashare-dev.pem ubuntu@34.219.222.117

cd /opt/back-end-b/

sudo git pull

sudo pm2 restart back-end-b

cd /opt/back-end-b/ && sudo git pull && sudo pm2 restart back-end-b

ssh -i ~/pem/slashare-dev.pem ubuntu@34.208.54.105

mysql -h dev-rds.dev.slashare.com -u root -p
Slasharemysql123

亚马逊账号
https://675508936409.signin.aws.amazon.com/console
用户名：jingfei
密码：Aa123456

飞书： https://da1wuf.axshare.com










[五款神器让Win10化身macOS](https://www.ithome.com/0/524/502.htm)